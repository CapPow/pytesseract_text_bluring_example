{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract as ocr\n",
    "from pytesseract import Output\n",
    "from glob import glob\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "# using joblib to schedule parallell tasks\n",
    "from joblib import Parallel, delayed, parallel_backend\n",
    "# multiprocessing is really only here to detect core counts.. could replace this with an explicit count\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ the destination for the source images will need to be altered. Also, it currently spits output images into the script's root directory. It may be wise to add in a clean destination. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    }
   ],
   "source": [
    "#available core count\n",
    "num_cores = multiprocessing.cpu_count() - 4\n",
    "#source images\n",
    "img_files = glob('../data_gathering/output/*.jpg')\n",
    "print(len(img_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Note:__ It is worth trying out different confidence values to fine tune ratio of Type I and type II errors. Currently indicated by the optional variable in \"ocr_gray_img.\" It may also be worth testing out some of the commented out tesseract configurations in same function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ocr_gray_img(img, thresh, conf=80):\n",
    "    \"\"\"\n",
    "    converts to grayscale at specified threshold and returns runs OCR\n",
    "    img = an opened cv2 image file\n",
    "    thresh = the value used in cv2 binary thresholding\n",
    "    conf = tesseract's minimum condience threshold when identifying text\n",
    "    \"\"\"\n",
    "    # run ocr on grayscale conversion\n",
    "    gray = cv2.threshold(img, thresh, 255, cv2.THRESH_BINARY)[1]\n",
    "    \n",
    "    # here I am testing many different tesseract configuration settings. It may be worth trying out different configs\n",
    "    \n",
    "    #config = \"--psm 13 --oem 3\"\n",
    "    #config = \"--dpi 300 --psm 11 --oem 1 -c tessedit_char_whitelist=0123456789\"\n",
    "    #config = \"--dpi 300 --psm 11 --oem 2 \\\n",
    "    #config = \"--dpi 1200 --psm 11 --oem 2 \\\n",
    "    #config = \"--dpi 1200 --psm 12 --oem 2 \\\n",
    "    config = \"--dpi 300 --psm 11 --oem 2\"\n",
    "#    config = \"--dpi 300 --psm 11 --oem 2 \\\n",
    "#            -c language_model_penalty_non_freq_dict_word=0.0 \\\n",
    "#            -c language_model_penalty_non_dict_word=0.0 \\\n",
    "#            -c language_model_penalty_punc=0.0 \\\n",
    "#            -c language_model_penalty_case=0.0 \\\n",
    "#            -c language_model_penalty_script=0.0 \\\n",
    "#            -c language_model_penalty_chartype=0.0 \\\n",
    "#            -c language_model_penalty_font=0.0 \\\n",
    "#            -c language_model_penalty_spacing=0.2\"\n",
    "    \n",
    "    ocr_boxes = ocr.image_to_data(gray, \n",
    "                                  output_type=Output.DICT,\n",
    "                                  lang=None,\n",
    "                                  config=config)\n",
    "\n",
    "    rot_img = cv2.rotate(img, cv2.cv2.ROTATE_90_COUNTERCLOCKWISE) \n",
    "    rot_ocr_boxes = ocr.image_to_data(rot_img, \n",
    "                                      output_type=Output.DICT,\n",
    "                                      lang=None,\n",
    "                                      config=config)\n",
    "\n",
    "    img_h, img_w = gray.shape[0:2]\n",
    "    # container to hold the results\n",
    "    boxes = []\n",
    "    # generate bounding box each text blob\n",
    "    # see: https://nanonets.com/blog/ocr-with-tesseract/\n",
    "    for box_group in [ocr_boxes, rot_ocr_boxes]:\n",
    "        # flag to determine if these are rotated boxes\n",
    "        rotated = box_group == rot_ocr_boxes\n",
    "        n_boxes = len(box_group['text'])\n",
    "        for i in range(n_boxes):\n",
    "            if int(box_group['conf'][i]) > conf:\n",
    "                (left, top, w, h) = (box_group['left'][i], box_group['top'][i], box_group['width'][i], box_group['height'][i])\n",
    "                # if the image was rotated 90deg ccw, correct the coords\n",
    "                if rotated:\n",
    "                    top_new = left\n",
    "                    left = img_w - (top+h)\n",
    "                    top = top_new\n",
    "                    h, w = w, h\n",
    "\n",
    "                # Set qualifying conditions for box size\n",
    "                width_thresh =  w < img_w * 0.4\n",
    "                height_thresh = h < img_h * 0.3\n",
    "                # if box size is reasonable, append it to results container\n",
    "                if (width_thresh & height_thresh):\n",
    "                    box = (top, left, w, h)\n",
    "                    boxes.append(box)\n",
    "\n",
    "    return boxes\n",
    "\n",
    "def apply_blur(img, boxes):\n",
    "    mask = np.zeros_like(img)\n",
    "    for box in boxes:\n",
    "        y, x, w, h = box\n",
    "        mask[y:y+h, x:x+w, ...] = [1, 1, 1]\n",
    "    blurred = cv2.GaussianBlur(img, (51, 51),30,30)\n",
    "    #blurred = cv2.GaussianBlur(img, (75, 75),100, 100)\n",
    "    output = np.where(mask, blurred, img)\n",
    "    \n",
    "    return output\n",
    "\n",
    "def merge_nearby_boxes(boxes, distance_thresh=10):    \n",
    "    # top, left, w, h = box\n",
    "    # container for final, joined rectangles\n",
    "    final_rects = []\n",
    "    toss_rects = [] # used to identify which ones are already merged into another.\n",
    "    for rect, next_rect in itertools.combinations(boxes, 2):    \n",
    "        keep_rect = rect # in case no other conditions trigger keep the unmerged rect.\n",
    "\n",
    "        if rect[1] < next_rect[1]:\n",
    "            left_most_rect = rect\n",
    "            left_least_rect = next_rect\n",
    "        else:\n",
    "            left_most_rect = next_rect\n",
    "            left_least_rect = rect\n",
    "            \n",
    "        if rect[0] < next_rect[0]:\n",
    "            top_most_rect = rect\n",
    "            top_least_rect = next_rect\n",
    "        else:\n",
    "            top_most_rect = next_rect\n",
    "            top_least_rect = rect\n",
    "\n",
    "        # establish overlapping conditionals\n",
    "        y_cond = (left_most_rect[1] + left_most_rect[2] + distance_thresh) >= left_least_rect[1]\n",
    "\n",
    "        x_cond = (top_most_rect[0] + top_most_rect[3] + distance_thresh) >= top_least_rect[0] \n",
    "\n",
    "        if (y_cond and x_cond):\n",
    "            w = (left_least_rect[1] + left_least_rect[2]) - left_most_rect[1]\n",
    "            h = (top_least_rect[0] + top_least_rect[3]) - top_least_rect[0]\n",
    "            keep_rect = (top_most_rect[0], left_most_rect[1], w, h)\n",
    "            toss_rects.append(rect)\n",
    "            toss_rects.append(next_rect)\n",
    "\n",
    "        final_rects.append(keep_rect)\n",
    "\n",
    "    toss_rects = set(toss_rects)\n",
    "    final_rects = set([x for x in final_rects if x not in toss_rects])\n",
    "\n",
    "    return final_rects\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = False\n",
    "\n",
    "for img_file in img_files:\n",
    "    img = cv2.imread(img_file)\n",
    "    boxes = []\n",
    "    \n",
    "    # contrast adjustment (see: https://stackoverflow.com/questions/42257173/contrast-stretching-in-python-opencv)\n",
    "    xp = [0, 64, 128, 192, 255]\n",
    "    fp = [0, 16, 128, 240, 255]\n",
    "    x = np.arange(256)\n",
    "    table = np.interp(x, xp, fp).astype('uint8')\n",
    "    cleaned = cv2.LUT(img, table)\n",
    "\n",
    "    for k in [0, 3, 5]:\n",
    "        # minor text cleaning before we ocr\n",
    "        if k > 0:\n",
    "            kernel = np.ones((k,k), np.uint8)\n",
    "            cleaned = cv2.erode(cleaned, kernel, iterations=1)\n",
    "            cleaned = cv2.dilate(cleaned, kernel, iterations=1)\n",
    "        # used to check out the cleaned image being handed to ocr\n",
    "        # establish an output file dest\n",
    "        if debug:\n",
    "            fn_w_ext = os.path.basename(img_file)\n",
    "            fn, ext = os.path.splitext(fn_w_ext)\n",
    "            new_fn = f\"{fn}_cleaned{ext}\"\n",
    "            cv2.imwrite(new_fn, cleaned)\n",
    "\n",
    "        # convert to grayscale at multiple thresholds and run OCR on each\n",
    "        with parallel_backend('threading'):\n",
    "            boxes.extend( Parallel(n_jobs=num_cores)(delayed(ocr_gray_img)(cleaned, i) for i in range(50, 200, 24)) )\n",
    "\n",
    "    # flatten the list of lists (one list of boxes per threshold) # reduce to unique elements\n",
    "    boxes = list(set(x for y in boxes for x in y))\n",
    "    # combine nearby boxes to blur interspaces\n",
    "    distance_thresh = int( min(img.shape[0:2]) * 0.05) #0.025\n",
    "    boxes = merge_nearby_boxes(boxes, distance_thresh)\n",
    "\n",
    "    # blur the area within each box \n",
    "    img = apply_blur(img, boxes)\n",
    "\n",
    "    # establish an output file dest\n",
    "    fn_w_ext = os.path.basename(img_file)\n",
    "    fn, ext = os.path.splitext(fn_w_ext)\n",
    "    new_fn = f\"{fn}_blurred{ext}\"\n",
    "    #save it\n",
    "    cv2.imwrite(new_fn, img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(boxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
